# Overview
## EdgeWiseLLM: 
### CUDA-Accelerated Large Language Model Inference with Configurable Performance and Optimal Decision-Making for Edge Devices
This project leverages pure C++ and CUDA technologies to unlock GPU acceleration potential and enhance inference performance on edge platforms. 

**EdgeWiseLLM** aims to tackle the challenges associated with deploying resource-intensive Large Language Models (LLMs) on edge devices, where hardware limitations severely restrict computational power and efficiency.
While Python-based libraries offer ease of use, they often fall short in fully harnessing GPU capabilities. 

Given the multitude of CUDA-based optimization techniques and diverse LLM architectural strategies, identifying the optimal inference configuration becomes a complex and time-consuming task.
EdgeWiseLLM addresses this by providing a configurable framework and a decision-making tool, enabling users to intuitively balance trade-offs in throughput, power consumption, bandwidth, memory usage, and model perplexity.
Thus, users can effectively and quickly tailor inference strategies to their specific hardware constraints and performance requirements.

The code and inspiration are based on [Link Text](https://github.com/andrewkchan/yalm)


# Methodology

# Results

# conclusion
